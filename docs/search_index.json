[["tm-eng.html", "2 Text Mining (Eng) 2.1 janeaustenr 2.2 gutenbergr", " 2 Text Mining (Eng) 2.1 janeaustenr 출처 : R로 배우는 텍스트 마이닝 제인 오스틴(Jane Austen)이 탈고해 출판한 소설 여섯 개를 janeaustenr 패키지 에서 가져온 다음 tidy 형식으로 변형해 보자. janeaustenr 패키지는 텍스트를 1줄당 1행(one-row-per-line) 형식으로 제공 mutate()를 사용해 linenumber 수에 해당하는 만큼을 주석으로 처리함으로써 원래 줄 형식을 추적하는데 사용 chapter를 사용해 모든 장이 어디부터 나오는지 찾아낸다 library(janeaustenr) library(dplyr) library(stringr) original_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() original_books #&gt; # A tibble: 73,422 x 4 #&gt; text book linenumber chapter #&gt; &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 &quot;SENSE AND SENSIBILITY&quot; Sense &amp; Sensibility 1 0 #&gt; 2 &quot;&quot; Sense &amp; Sensibility 2 0 #&gt; 3 &quot;by Jane Austen&quot; Sense &amp; Sensibility 3 0 #&gt; 4 &quot;&quot; Sense &amp; Sensibility 4 0 #&gt; 5 &quot;(1811)&quot; Sense &amp; Sensibility 5 0 #&gt; 6 &quot;&quot; Sense &amp; Sensibility 6 0 #&gt; 7 &quot;&quot; Sense &amp; Sensibility 7 0 #&gt; 8 &quot;&quot; Sense &amp; Sensibility 8 0 #&gt; 9 &quot;&quot; Sense &amp; Sensibility 9 0 #&gt; 10 &quot;CHAPTER 1&quot; Sense &amp; Sensibility 10 1 #&gt; # ... with 73,412 more rows 이것을 tidy 데이터셋으로 사용하려면 unnest_tokens() 함수를 사용해 1행당 1토큰(one-token-per-row) 형식으로 구성해야 한다. library(tidytext) tidy_books &lt;- original_books %&gt;% unnest_tokens(word, text) tidy_books #&gt; # A tibble: 725,055 x 4 #&gt; book linenumber chapter word #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Sense &amp; Sensibility 1 0 sense #&gt; 2 Sense &amp; Sensibility 1 0 and #&gt; 3 Sense &amp; Sensibility 1 0 sensibility #&gt; 4 Sense &amp; Sensibility 3 0 by #&gt; 5 Sense &amp; Sensibility 3 0 jane #&gt; 6 Sense &amp; Sensibility 3 0 austen #&gt; 7 Sense &amp; Sensibility 5 0 1811 #&gt; 8 Sense &amp; Sensibility 10 1 chapter #&gt; 9 Sense &amp; Sensibility 10 1 1 #&gt; 10 Sense &amp; Sensibility 13 1 the #&gt; # ... with 725,045 more rows 이 함수는 tokenizers를 사용해 원래 데이터 프레임에 있는 텍스트의 각 행을 토큰으로 분리한다. 기본 토큰화는 단어에 대한 것이지만 다른 옵션을 사용하면 문자, 엔그램, 문장, 줄, 단락 단위로 토큰호하 할 수 있고, 또는 정규 표혀노식 패턴을 사용해서 분리할 수 있다. 불용어(stop words)는 분석에 유용하지 않은 단어들을 말하며, 일반적으로 영어의 ‘the,’ ‘of,’ ‘to’ 등과 같은 매우 전형적인 단어를 말한다. anti_join()을 사용해 불용어를 제거할 수 있다. 이 때, 사용되는 불용어는 stop_words를 사용한다. data(stop_words) tidy_books &lt;- tidy_books %&gt;% anti_join(stop_words) tidy_books #&gt; # A tibble: 217,609 x 4 #&gt; book linenumber chapter word #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Sense &amp; Sensibility 1 0 sense #&gt; 2 Sense &amp; Sensibility 1 0 sensibility #&gt; 3 Sense &amp; Sensibility 3 0 jane #&gt; 4 Sense &amp; Sensibility 3 0 austen #&gt; 5 Sense &amp; Sensibility 5 0 1811 #&gt; 6 Sense &amp; Sensibility 10 1 chapter #&gt; 7 Sense &amp; Sensibility 10 1 1 #&gt; 8 Sense &amp; Sensibility 13 1 family #&gt; 9 Sense &amp; Sensibility 13 1 dashwood #&gt; 10 Sense &amp; Sensibility 13 1 settled #&gt; # ... with 217,599 more rows tidytext 패키지의 stop_words 데이터셋에는 3개의 불용어 용어집(lexicon)이 들어있다. 지금처럼 모두 함께 사용할 수도 있고, 특정 분석에 더 적합한 경우 filter()를 사용해 1개 불용어 집합만 사용할 수도 있다. 또한, dplyr의 count()를 사용해 모든 도서에서 가장 흔하게 나오는 단어를 찾을 수 있다. tidy_books %&gt;% count(word, sort = TRUE) #&gt; # A tibble: 13,914 x 2 #&gt; word n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 miss 1855 #&gt; 2 time 1337 #&gt; 3 fanny 862 #&gt; 4 dear 822 #&gt; 5 lady 817 #&gt; 6 sir 806 #&gt; 7 day 797 #&gt; 8 emma 787 #&gt; 9 sister 727 #&gt; 10 house 699 #&gt; # ... with 13,904 more rows 단어 카운트(word count) 결과는 tidy data frame에 저장되었기 때문에 아래처럼 ggplot2 패키지로 직접 연결(pipe)할 수 있습니다 (Figure 2.1). library(ggplot2) tidy_books %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 600) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) Figure 2.1: The most common words in Jane Austen’s novels 2.2 gutenbergr gutenbergr 패키지는 구텐베르크 프로젝트 모음집 중 공공 저작물에 해당하는 텍스트에 접근할 수 있게 한다. 이 패키지에는 도서를 내려받기 위한 도구와 관심있는 작품을 찾는데 사용할 수 있는 구텐베르크 프로젝트 메타데이터의 전체 데이터셋이 포함되어 있다. gutenbergr에 대한 자세한 내용은 rOpenSci의 패키지 튜토리얼(https://ropensci.org/tutorials/gutenbergr_tutorial/)을 참조하자. 2.2.1 단어 빈도 먼저 19세기 말부터 20세기 초반에 걸쳐 살았던 웰스의 공상 과학 소설과 판타지 소설을 살펴보자. The Time Machine The War of the Worlds The Invisible Man The Island of Doctor Moreau 우리는 gutenberg_download()와 각 소설에 대한 구텐베르크 프로젝트 식별 번호를 사용해 이러한 작품에 액세스할 수 있다. library(gutenbergr) hgwells &lt;- gutenberg_download(c(35, 36, 5230, 159)) tidy_hgwells &lt;- hgwells %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) 재미 삼아서 웰스의 소설에 가장 공통적으로 나오는 단어는 무엇인지 알아보자. tidy_hgwells %&gt;% count(word, sort = TRUE) #&gt; # A tibble: 11,811 x 2 #&gt; word n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 time 461 #&gt; 2 people 302 #&gt; 3 door 260 #&gt; 4 heard 249 #&gt; 5 black 232 #&gt; 6 stood 229 #&gt; 7 white 224 #&gt; 8 hand 218 #&gt; 9 kemp 213 #&gt; 10 eyes 210 #&gt; # ... with 11,801 more rows 이번에는 브론테(Bronte) 자매의 유명한 작품을 입수해 볼 텐데, 브론테 잗매는 제인 오스틴과 비슷한 시대를 살았지만 오히려 다른 문체로 글을 썼다. Jane Eyre Wuthering Heights The Tenant of Wildfell Hall Villette Agnes Grey 각 소설에 구텐베르크 프로젝트 식별 번호를 다시 사용하고 gutenberg_download()를 사용하면 해당 텍스트에 액세스할 수 있다. bronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 767)) tidy_bronte &lt;- bronte %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) 브론테 자매의 소설에서 가장 자주 출현하는 단어는 무엇인가? tidy_bronte %&gt;% count(word, sort = TRUE) #&gt; # A tibble: 23,297 x 2 #&gt; word n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 time 1065 #&gt; 2 miss 854 #&gt; 3 day 825 #&gt; 4 hand 767 #&gt; 5 eyes 714 #&gt; 6 don’t 666 #&gt; 7 night 648 #&gt; 8 heart 638 #&gt; 9 looked 601 #&gt; 10 door 591 #&gt; # ... with 23,287 more rows 우레스와 브론테 자매의 작품들에서 모두 “time,” “eyes,” and “hand” 라는 단어가 상위 10위 안에 속 “time,” “ey”time“,”ey Brontë sisters. 이제 제인 오스틴, 브론테 자매 및 웰스의 작품에서 각 단어의 빈도를 계산해 보고 데이터 프레임을 함께 묶자. 우리는 tidyr의 spread()와 gather()를 사용해 데이터 프레임을 재구성함으로써 세 개의 소설을 그려서 비교하기에 알맞게 한다. library(tidyr) frequency &lt;- bind_rows(mutate(tidy_bronte, author = &quot;Brontë Sisters&quot;), mutate(tidy_hgwells, author = &quot;H.G. Wells&quot;), mutate(tidy_books, author = &quot;Jane Austen&quot;)) %&gt;% mutate(word = str_extract(word, &quot;[a-z&#39;]+&quot;)) %&gt;% count(author, word) %&gt;% group_by(author) %&gt;% mutate(proportion = n / sum(n)) %&gt;% select(-n) %&gt;% pivot_wider(names_from = author, values_from = proportion) %&gt;% pivot_longer(`Brontë Sisters`:`H.G. Wells`, names_to = &quot;author&quot;, values_to = &quot;proportion&quot;) frequency #&gt; # A tibble: 57,116 x 4 #&gt; word `Jane Austen` author proportion #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a 0.00000919 Bronte Sisters 0.0000587 #&gt; 2 a 0.00000919 H.G. Wells 0.0000147 #&gt; 3 aback NA Bronte Sisters 0.00000391 #&gt; 4 aback NA H.G. Wells 0.0000147 #&gt; 5 abaht NA Bronte Sisters 0.00000391 #&gt; 6 abaht NA H.G. Wells NA #&gt; 7 abandon NA Bronte Sisters 0.0000313 #&gt; 8 abandon NA H.G. Wells 0.0000147 #&gt; 9 abandoned 0.00000460 Bronte Sisters 0.0000900 #&gt; 10 abandoned 0.00000460 H.G. Wells 0.000177 #&gt; # ... with 57,106 more rows 구텐베르그 프로젝트 내 UTF-8로 인코딩된 텍스트에 밑줄을 쳐서 강조한(기울임 꼴처럼) 몇 가지 사례가 들어 있으므로 여기서는 str_extract()를 사용할 것이다. tokenizer는 이렇게 강조 처리한 단어를 강조 처리를 하지 않은 단어와 서로 다른 단어로 여기지만, str_extract()를 사용해 보기 전에 우리가 초기 데이터 탐색 작업을 할 때에 보았듯이 우리는 ’any’와 ’any’를 따로 세기를 원하지 않는다. 이제 그래프를 그려보자(Figure 2.2). library(scales) # 결측값(missing values)이 제거된 행(row)에 대한 경고가 표시될 수 있다. ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) + geom_abline(color = &quot;gray40&quot;, lty = 2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + scale_color_gradient(limits = c(0, 0.001), low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) + facet_wrap(~author, ncol = 2) + theme(legend.position=&quot;none&quot;) + labs(y = &quot;Jane Austen&quot;, x = NULL) Figure 2.2: Comparing the word frequencies of Jane Austen, the Bronte sisters, and H.G. Wells 이 그림들 중에서 점선에 가까운 단어는 두 텍스트 집합에서 모두 비슷한 빈도를 보이는데, 예를 들면 오스틴과 브론테의 텍스트를 비교한 경우(빈도가 높은 상단에 ‘miss,’ ‘tiem,’ ‘day’가 있음)와 오스틴과 웰스의 텍스트를 비교한 경우(빈도가 높은 상단에 ’time,’ ‘day,’ ‘brother’가 있음)를 들 수 있다. 점선에서 멀리 떨어져 있는 단어는 어느 한쪽 텍스트 집합에서만 더 많이 발견되는 단어다. 예를 들어 오스틴-브론테 비교 그림에서 ’elizabeth,’ ‘anne,’ ‘fanny’와 같은 단어(모두 고유 명사)는 오스틴의 텍스트에는 많지만 브론테 텍스트에는 많지 않고, ’arthur’와 ’dog’는 브론테 텍스트에는 있지만 오스틴 텍스트에는 없다. 웰스를 제인 오스틴과 비교해 보면 웰스가 쓰는 ’beast,’ ‘guns,’ ‘feet,’ ‘black’과 같은 단어를 오스틴은 쓰지 않는 반면에, 오스틴이 사용하는 ’family,’ ‘friend,’ ‘letter,’ ’dear’와 같은 단어를 웰스는 쓰지 않는다. 전반적으로 그림에서 오스틴-브론테 비교 그래프에 나오는 단어들이 오스틴-웰스 비교 그래프에 나오는 것보다 기울게 그어진 점선에 더 가까이 있다는 점에 주목하자. 또한 단어가 오스틴-브론테 그래프에서 빈도가 더 낮은 곳까지 퍼져 있다는 점에 주목하자. 오스텐-웰스 비교 그래프에서 빈도가 낮은 곳이 비어있다. 이러한 특성은 오스틴이 웰스보다는 브론테와 더 비슷한 단어를 사용함을 나타낸다. 또한 우리는 모든 단어가 세 가지 집합으로 이뤄진 텍스트 모두에 나오지는 않는다는 점과, 오스틴과 웰스의 빈도를 나타낸 그래프에 데이터 점이 적다는 것을 알 수 있다. 이러한 단어 집합의 유사성과 차이점을 상관 검정을 통해 정량화(quantify, 수량화)해 보자. 오스틴과 브론테 자매 사이, 오스틴과 웰스 사이의 단어 빈도는 어떤 상관(correlation)이 있는가? cor.test(data = frequency[frequency$author == &quot;Brontë Sisters&quot;,], ~ proportion + `Jane Austen`) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: proportion and Jane Austen #&gt; t = 111.06, df = 10346, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.7285370 0.7461189 #&gt; sample estimates: #&gt; cor #&gt; 0.7374529 cor.test(data = frequency[frequency$author == &quot;H.G. Wells&quot;,], ~ proportion + `Jane Austen`) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: proportion and Jane Austen #&gt; t = 35.229, df = 6008, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.3925914 0.4345047 #&gt; sample estimates: #&gt; cor #&gt; 0.4137673 우리가 그래프에서 볼 수 있었듯이 오스틴과 웰스 사이보다는 오스틴과 브론테 소설 사이에서 단어의 빈도가 더 많이 연관되어 있다. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
