[["text-mining-eng.html", "2 Text Mining (Eng) 2.1 제인 오스틴(Jane Austen) 작품 분석 2.2 Ref.", " 2 Text Mining (Eng) 2.1 제인 오스틴(Jane Austen) 작품 분석 제인 오스틴(Jane Austen)이 탈고해 출판한 소설 여섯 개를 janeaustenr 패키지 에서 가져온 다음 tidy 형식으로 변형해 보자. janeaustenr 패키지는 텍스트를 1줄당 1행(one-row-per-line) 형식으로 제공 mutate()를 사용해 linenumber 수에 해당하는 만큼을 주석으로 처리함으로써 원래 줄 형식을 추적하는데 사용 chapter를 사용해 모든 장이 어디부터 나오는지 찾아낸다 library(janeaustenr) library(dplyr) library(stringr) original_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() original_books #&gt; # A tibble: 73,422 x 4 #&gt; text book linenumber chapter #&gt; &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 &quot;SENSE AND SENSIBILITY&quot; Sense &amp; Sensibility 1 0 #&gt; 2 &quot;&quot; Sense &amp; Sensibility 2 0 #&gt; 3 &quot;by Jane Austen&quot; Sense &amp; Sensibility 3 0 #&gt; 4 &quot;&quot; Sense &amp; Sensibility 4 0 #&gt; 5 &quot;(1811)&quot; Sense &amp; Sensibility 5 0 #&gt; 6 &quot;&quot; Sense &amp; Sensibility 6 0 #&gt; 7 &quot;&quot; Sense &amp; Sensibility 7 0 #&gt; 8 &quot;&quot; Sense &amp; Sensibility 8 0 #&gt; 9 &quot;&quot; Sense &amp; Sensibility 9 0 #&gt; 10 &quot;CHAPTER 1&quot; Sense &amp; Sensibility 10 1 #&gt; # ... with 73,412 more rows 이것을 tidy 데이터셋으로 사용하려면 unnest_tokens() 함수를 사용해 1행당 1토큰(one-token-per-row) 형식으로 구성해야 한다. library(tidytext) tidy_books &lt;- original_books %&gt;% unnest_tokens(word, text) tidy_books #&gt; # A tibble: 725,055 x 4 #&gt; book linenumber chapter word #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Sense &amp; Sensibility 1 0 sense #&gt; 2 Sense &amp; Sensibility 1 0 and #&gt; 3 Sense &amp; Sensibility 1 0 sensibility #&gt; 4 Sense &amp; Sensibility 3 0 by #&gt; 5 Sense &amp; Sensibility 3 0 jane #&gt; 6 Sense &amp; Sensibility 3 0 austen #&gt; 7 Sense &amp; Sensibility 5 0 1811 #&gt; 8 Sense &amp; Sensibility 10 1 chapter #&gt; 9 Sense &amp; Sensibility 10 1 1 #&gt; 10 Sense &amp; Sensibility 13 1 the #&gt; # ... with 725,045 more rows 이 함수는 tokenizers를 사용해 원래 데이터 프레임에 있는 텍스트의 각 행을 토큰으로 분리한다. 기본 토큰화는 단어에 대한 것이지만 다른 옵션을 사용하면 문자, 엔그램, 문장, 줄, 단락 단위로 토큰호하 할 수 있고, 또는 정규 표혀노식 패턴을 사용해서 분리할 수 있다. 불용어(stop words)는 분석에 유용하지 않은 단어들을 말하며, 일반적으로 영어의 ‘the,’ ‘of,’ ‘to’ 등과 같은 매우 전형적인 단어를 말한다. anti_join()을 사용해 불용어를 제거할 수 있다. 이 때, 사용되는 불용어는 stop_words를 사용한다. data(stop_words) tidy_books &lt;- tidy_books %&gt;% anti_join(stop_words) tidytext 패키지의 stop_words 데이터셋에는 3개의 불용어 용어집(lexicon)이 들어있다. 지금처럼 모두 함께 사용할 수도 있고, 특정 분석에 더 적합한 경우 filter()를 사용해 1개 불용어 집합만 사용할 수도 있다. 또한, dplyr의 count()를 사용해 모든 도서에서 가장 흔하게 나오는 단어를 찾을 수 있다. tidy_books %&gt;% count(word, sort = TRUE) #&gt; # A tibble: 13,914 x 2 #&gt; word n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 miss 1855 #&gt; 2 time 1337 #&gt; 3 fanny 862 #&gt; 4 dear 822 #&gt; 5 lady 817 #&gt; 6 sir 806 #&gt; 7 day 797 #&gt; 8 emma 787 #&gt; 9 sister 727 #&gt; 10 house 699 #&gt; # ... with 13,904 more rows 단어 카운트(word count) 결과는 tidy data frame에 저장되었기 때문에 아래처럼 ggplot2 패키지로 직접 연결(pipe)할 수 있습니다 (Figure 2.1). library(ggplot2) tidy_books %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 600) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) Figure 2.1: The most common words in Jane Austen’s novels 2.2 Ref. janeaustenr tokenizers "]]
